- [Site Reliability Engineering](#site-reliability-engineering)
  * [Part I Introduction](#part-i-introduction)
    + [Chapter 1 Introduction](#chapter-1-introduction)
    + [Chapter 2 The Production Environment at Google, from the Viewpoint of an SRE](#chapter-2-the-production-environment-at-google-from-the-viewpoint-of-an-sre)
    + [Chapter 3 Embracing Risk](#chapter-3-embracing-risk)
  * [Part II Principles](#part-ii-principles)
    + [Chapter 4 Service Level Objectives](#chapter-4-service-level-objectives)
    + [Chapter 5 Eliminating Toil](#chapter-5-eliminating-toil)
    + [Chapter 6 Monitoring Distributed Systems](#chapter-6-monitoring-distributed-systems)
    + [Chapter 7 The Evolution of Automation at Google](#chapter-7-the-evolution-of-automation-at-google)
    + [Chapter 8 Release Engineering](#chapter-8-release-engineering)
    + [Chapter 9 Simplicity](#chapter-9-simplicity)
  * [Part III - Practices](#part-iii---practices)
    + [Chapter 10 Practical Alerting](#chapter-10-practical-alerting)
    + [Chapter 11 Being On-Call](#chapter-11-being-on-call)
    + [Chapter 12 Effective Troubleshooting](#chapter-12-effective-troubleshooting)
    + [Chapter 13 Emergency Response](#chapter-13-emergency-response)
    + [Chapter 14 Managing Incidents](#chapter-14-managing-incidents)
    + [Chapter 15 Postmortem Culture: Learning from Failure](#chapter-15-postmortem-culture-learning-from-failure)
    + [Chapter 16 Tracking Outages](#chapter-16-tracking-outages)
    + [Chapter 17 Testing for Reliability](#chapter-17-testing-for-reliability)
    + [Chapter 18 Software Engineering in SRE](#chapter-18-software-engineering-in-sre)
    + [Chapter 19 Load Balancing at the Frontend](#chapter-19-load-balancing-at-the-frontend)
    + [Chapter 20 Load Balancing in the Datacenter](#chapter-20-load-balancing-in-the-datacenter)
    + [Chapter 21 Handling Overload](#chapter-21-handling-overload)
    + [Chapter 22 Addressing Cascading Failures](#chapter-22-addressing-cascading-failures)
    + [Chapter 23 Managing Critical State: Distributed Consensus for Reliability](#chapter-23-managing-critical-state-distributed-consensus-for-reliability)
    + [Chapter 24 Distributed Periodic Scheduling with Cron](#chapter-24-distributed-periodic-scheduling-with-cron)
    + [Chapter 25 Data Processing Pipelines](#chapter-25-data-processing-pipelines)
    + [Chapter 26 Data Integrity: What You Read Is What You Wrote](#chapter-26-data-integrity-what-you-read-is-what-you-wrote)
    + [Chapter 27 Reliable Product Launches at Scale](#chapter-27-reliable-product-launches-at-scale)
  * [Part IV - Management](#part-iv---management)
    + [Chapter 28 Accelerating SREs to On-Call and Beyond](#chapter-28-accelerating-sres-to-on-call-and-beyond)
    + [Chapter 29 Dealing with Interrupts](#chapter-29-dealing-with-interrupts)
    + [Chapter 30 Embedding an SRE to Recover from Operational Overload](#chapter-30-embedding-an-sre-to-recover-from-operational-overload)
    + [Chapter 31 Communication and Collaboration in SRE](#chapter-31-communication-and-collaboration-in-sre)
    + [Chapter 32 The Evolving SRE Engagement Model](#chapter-32-the-evolving-sre-engagement-model)
  * [Part V - Conclusions](#part-v---conclusions)
    + [Chapter 33 Lessons Learned from Other Industries](#chapter-33-lessons-learned-from-other-industries)
    + [Chapter 34 Conclusion](#chapter-34-conclusion)
___

# Site Reliability Engineering

## Part I Introduction

### Chapter 1 Introduction

- [text](https://sre.google/sre-book/introduction/)
- historically, companies have employed systems administrators to run complex
  computing system
  * product developers and sysadmins are divided into discrete teams
    + development
    + operations
  * advantages
    + integration companies are available to help run those assembled systems,
      so a novice sysadmin team does not have to reinvent the wheel and design
      a system from scratch
  * disadvantages
    + direct costs
      + running a service with a team that relies on manual intervention for
        both change management and event handling becomes expensive as the
        service and/or traffic to the service grows, because the size of the
        team necessarily scales with the load generated by the system
    + indirect costs
      + development and operations team are quite different in background, skill
        set and incentives; they use different vocabulary to describe
        situations; they carry different assumptions about both risk and
        possibilities for technical solutions; they have different assumptions
        about the target level of product stability; they split between the
        groups can easily become one of not just incentives, but also
        communication, goals, and eventually, trust and respect; this outcome is
        a pathology

### Chapter 2 The Production Environment at Google, from the Viewpoint of an SRE

- [text](https://sre.google/sre-book/production-environment/)
- a single shared repository
  * engineers encounter a problem in a componment (or service) outside their
    project, they can fix the problem, send a pull request to the owner for
    merge the pull request to production

## Part II Principles

- [text](https://sre.google/sre-book/part-II-principles/)
- the industry commonly lumps disparate concepts under the general banner of
  SLA, a tendency that makes it harder to think about these concepts clearly;
  SLO attempts to disentangle indicators from objectives from agreements

### Chapter 3 Embracing Risk

- [text](https://sre.google/sre-book/embracing-risk/)
- extreme reliability comes at a cost: maximising stability limits how fast new
  features can be developed and how quickly products can be delivered to users,
  and dramatically increases their cost, which in turn reduces the number of
  features a team can afford to offer
- examples of risk can be taken
  * a user on a 99% reliable smartphone cannot tell the difference between
    99.99% and 99.999% service reliability
- site reliability engineering seeks to balance the risk of unavailability with
  the goals of rapid innovation and efficient service operations, so that users’
  overall happiness—with features, service, and performance—is optimized
- costs of reducing too much risk
  * cost of redundant compute resources
  * opportunity cost
    + the cost borne by an organization when it allocates engineering resources
      to build systems or features that diminish risk instead of features that
      are directly visible to or usable by end users
- drawbacks of using time-based availability
  * since platforms these days are seldom made of only one service, time-based
    availability may not be meaningful in case of all services but one is down;
    this "partially" up nature make time-based availability less useful
- aggregated availability
  * a yield-based metric calculated over a rolling window
  * quantifying unplanned downtime as a request success rate also makes this
    availability metric more amenable for use in systems that do not typically
    serve end users directly
    + examples
      + using a request success rate defined in terms of records successfully
        and unsuccessfully processed, we can calculate a useful availability
        metric despite the fact that the batch system does not run constantly
  * quarterly availability targets for a service can be set and it can be used
    to track performance against those targets on a weekly, or even daily, basis
- risk tolerance of services is typically built directly into the basic product
  or service definition
- to identify the risk tolerance of consumer services
  * target level of availability
    + an external quarterly target can be set and back this target with
      a stronger internal availability target
  * types of failures
    + examples
      + a constant low rate of failures
      + an occasional full-site outage
    + planned downtime
    + unplanned downtime
  * cost
  * other service metrics
    + examples
      + latency
- motivation for error budgets
  * different incentives from teams
    + product development
      + performance is largely evaluated on product velocity, which creates an
        incentive to push new code as quickly as possible
    + SRE
      + performance is evaluated based upon reliability of a service, which
        implies an incentive to push back against a high rate of change
- factors to consider in determining error budgets
  * software fault tolerance
    + how hardened do we make the software to unexpected events?
  * testing
    + not enough testing and you have embarrassing outages, privacy data leaks,
      or a number of other press-worthy events; testing too much may get into
      a problem of moving too slow in terms of product development
  * push frequency
  * canary duration and size
    + it is a best practice to test a new release on some small subset of
      a typical workload, a practice often called canarying. How long do we
      wait, and how big is the canary?
  * usually, preexisting teams have worked out some kind of informal balance
    between them as to where the risk/effort boundary lies; unfortunately, one
    can rarely prove that this balance is optimal, rather than just a function
    of the negotiating skills of the engineers involved; nor should such
    decisions be driven by politics, fear, or hope
- forming an error budget
  * application development and SRE team jointly define a quarterly error budget
    based on the SLO of the service
  * this metric removes the politics from negotiations between SREs and
    application developers when deciding how much risk to allow
  * use caess
    + as long as there is error budget remaining, new releases can be pushed
    + slowing down releases or rolling releases back when SLO-violation error
      budget is close to being used up
    + if product development wants to skimp on testing or increase push velocity
      and SRE is resistant, the error budget guides the decision
      + when the budget is large, the product developers can take more risks
      + when the budget is nearly drained, the product developers themselves
        will push for more testing or slower push velocity, as they do not want
        to risk using up the budget and stall their launch
      + in effect, the product development team becomes self-policing
    + if a network outage or datacenter failure reduces the measured SLO
      + the number of new pushes may be reduced for the remainder of the quarter
      + the entire team supports this reduction because everyone shares the
        responsibility for uptime
      + this make sense because the budget reflects what end-user is
        experiencing and further risk can be taken has been reduced due to the
        outage

### Chapter 4 Service Level Objectives

- [text](https://sre.google/sre-book/service-level-objectives/)
- SLI
  * a carefully defined quantitative measure of some aspect of the level of
    service that is provided
  * examples
    + request latency
    + error rate
    + throughput as request per second
    + availability as the fraction of the time that a service is usable
      + yield
        + the fraction of well-formed requests that succeed
      + often referred to as number of 9s
  * proxy measures is used as direct measurements are hard to obtain or
    interpret
    + examples
      + server side latency as a proxy for client side latency
- SLO
  * a target value or range of values for a service level that is measured by an
    SLI
  * examples
    + SLI ≤ target
    + lower bound ≤ SLI ≤ upper bound
  * choosing and publishing SLOs to users sets expectations about how a service
    will perform
    + this strategy can reduce unfounded complaints to service owners about, for
      example, the service being slow
  * it can and should be a major driver in prioritising work for SREs and
    prouct developers because they reflect what users care about
- SLA
  * an overloaded term and it has taken on a number of meanings depending on
    context
  * an explicit or implicit contract with your users that includes consequences
    of meeting (or missing) the SLOs they contain
  * the consequences are most easily recognized when they are financial but they
    can take other forms
  * an easy way to tell the difference between an SLO and an SLA is to ask "what
    happens if the SLOs aren’t met?"
    + if there is no explicit consequence, then you are almost certainly looking
    + at an SLO
  * SRE does not typically get involved in constructing SLAs, because SLAs are
    closely tied to business and product decisions
  * whether or not a particular service has an SLA, it is valuable to define
    SLIs and SLOs and use them to manage the service
- relavant SLIs
  * not every metric should be used as an SLI
  * choosing too many indicators makes it hard to pay the right level of
    attention to the indicators that matter, while choosing too few may leave
    significant behaviors of your system unexamined
  * an understanding from user's perspective will inform the judicious selection
    of a few indicators
  * examples
    + user-facing serving systems
      + availability, latency, throughput
    + big data systems (or business intelligence systems)
      + throughput, end-to-end latency (how long does it take the data from
        ingestion to completion)
    + all systems
      + correctness
- percentiles
  * a high-order percentile (such as 99th) shows a plausible worst-case
    experience for a user
  * median (50th) emphasizes the typical experience
- statistics
  * it should not be assumed without verifying that metrics collected are
    normally distributed
- practical strategies
  * start by thinking about or finding out what your users care about, not what
    you can measure
  * defining objectives
    + specify how indicators are measured and the conditions under which the
      indicators are valid
    + basic examples
      + 99% (averaged over 1 minute) of `GET` RPC calls will complete in less
        than 100 ms (measured across all the backend servers
      + 99% of `GET` RPC calls will complete in less than 100 ms (this is the
        same condition but with default SLI assumptions made)
    + examples when shape of performance curves are important
      + 90% of `GET` RPC calls will complete in less than 1 ms
      + 99% of `GET` RPC calls will complete in less than 10 ms
      + 99.9% of `GET` RPC calls will complete in less than 100 ms
    + examples on heterogeneous workloads
      + 95% of throughput clients’ Set RPC calls will complete in < 1 s
      + 99% of latency clients’ Set RPC calls with payloads < 1 kB will complete
        in < 10 ms
  * error budgets
    + a rate at which the SLOs can be missed and track that on a daily or weekly
      basis
    + upper management will probably want a monthly or quarterly assessment
    + an error budget is just an SLO for meeting other SLOs
  * do not pick a target based on current performance
    + while understanding the merits and limits of a system is essential,
      adopting values without reflection may lock you into supporting a system
      that requires heroic efforts to meet its targets, and that cannot be
      improved without significant redesign
  * keep it simple
    + complicated aggregations in SLIs can obscure changes to system
      performance, and are also harder to reason about
  * avoid absolutes
    + while it is tempting to ask for a system that can scale its load
      "infinitely" without any latency increase and that is "always" available,
      this requirement is unrealistic; even a system that approaches such ideals
      will probably take a long time to design and build, and will be expensive
      to operate—and probably turn out to be unnecessarily better than what
      users would be happy (or even delighted) to have
  * have a few SLOs as possible
    + choose just enough SLOs to provide good coverage of your system’s
      attributes
    + defend the SLOs you pick
      + if you cannot ever win a conversation about priorities by quoting
        a particular SLO, it is probably not worth having that SLO
  * perfection can wait
    + you can always refine SLO definitions and targets over time as you learn
      about a system’s behavior
      + it is better to start with a loose target that you tighten than to
        choose an overly strict target that has to be relaxed when you discover
        it is unattainable
  * publishing SLOs
    + publishing SLOs sets expectations for system behavior; users (and
      potential users) often want to know what they can expect from a service in
      order to understand whether it is appropriate for their use case
  * keep a safety margin
    + using a tighter internal SLO than the SLO advertised to users gives you
      room to respond to chronic problems before they become visible externally;
      an SLO buffer also makes it possible to accommodate reimplementations that
      trade performance for other attributes, such as cost or ease of
      maintenance, without having to disappoint users
  * do not overachieve
    + users build on the reality of what you offer, rather than what you say you
      will supply, particularly for infrastructure services; if the actual
      performance is much better than its stated SLO, users will come to rely on
      its current performance; you can avoid over-dependence by throttling some
      requests, or designing the system so that it is not faster under light
      loads

### Chapter 5 Eliminating Toil

- [text](https://sre.google/sre-book/eliminating-toil/)
- toil
  * to avoid using "operational work" which can be easily misinterpreted
  * what is not toil
    + overhead
      + examples
        + administrative chores
        + team meetings
        + team setting
        + grading goals
        + snippets
        + HR paperwork
    + grungy work which has long-term value
      + examples
        + cleaning up the entire alerting configuration for a service and
          rmoving clutter
  * not every task deemed toil has all the above characteristics, but the more
    closely work matches one or more of the following descriptions, the more
    likely it is to be toil
  * characteristics
    + manual
      + running a script that automates some tasks
    + repetitive
    + automatable
      + however, if human judgment is essentualfor the task, it is likely not
        toil
    + tactical
      + interrupt-driven and reactive
        + rather than strategy-driven and proactive
      + examples
        + handling pager alerts
    + no enduring value
      + if the service remians in the same state after a task has been
        completed, the task was likely toil
        + if the ask produced a permanent improvement in the service, it
          probably was not toil
          + even if some amount of grunt work, such as digging into legacy code
            and configurations and straightening them out, was involved
    + scales linearly as a service grows
      + if the task scales up linearly with the service size, traffic volume of
        user count
- activities of SRE
  * software engineering
  * systems engineering
    + examples
      + monitoring setup
      + consulting on architecture, design and productionisation for developer
        teams
  * toil
  * overhead
- some toil in SRE work is unavoidable
  * it becomes a problem when experienced in large quantities
  * some toil tasks can be low-risk and low-stress activities
    + predictable and repetitive tasks can be quite calming and some people
      gravitate toward this type of work
- potential problems with toil
  * career stagnation
    + too little spend on learning and improving system
  * creates confusion
    + individuals or teams within SRE that engage in too much toil undermine the
      clarity of that communication and confuse people about the SRE role
  * slows progress
  * sets precedent
    + if one engineer is too willing to take on toil, the team will have
      incentives to load the engineer with even more toil, sometimes shifting
      operational tasks that should be rightfully be performed by developers to
      SRE
  * promotes attrition
    + the best engineers to start looking elsewhere for a more rewarding job
  * cases breach of faith
    + new hires or transfers who joined SRE with promise of project will feel
      cheated, which is bad for morale

### Chapter 6 Monitoring Distributed Systems

- [text](https://sre.google/sre-book/monitoring-distributed-systems/)
- reasons to monitor
  * analysing long-term trends
  * comparing over time or experiment groups
  * alerting
  * building dashboards
  * conducting ad hoc retrospective analysis
    + that is, debugging
    + examples
      + debugging for an increase of latency and to find out how other
        components or services were behaving at the time
- alerting
  * it should not be done simply because "something seems a bit weird"
  * paging
    + it is a quite expensive user of an employee's time
      + during office hours
        + interrupts their workflow
      + during non-office hours
        + interrupts their personal time and even their sleep
      + too frequent leads to employee second-guess, skim, or event ignore the
        paging
        + risk masking out the "real" page
        + outages can be prolonged because other noises interferes with a repid
          diagnosis and fix
    + a good signal to very low noise is important
  * rules that generate alerts for humans should be simple to understand and
    represent a clear failure
- aims of a monitoring systems
  * addressing what is broken
  * addressing why it is broken
- white-box monitoring
  * based on metrics exposed by the internals of the system (or applications
    running on the system), including logs, or an HTTP handler that emits
    internal statistics
  * detection of imminent problems, failures masked by retries and so forth
  * debugging is one of the important use-cases
- black-box monitoring
  * testing externally visible hebaviour as a user would see it
  * symptom-oriented and represents active (not predicted) problems
  * paging is one of the major use-cases
    + as black-box monitoring covers mostly the "unexpected" problems
- monitoring at Google
  * heavy use of white-box monitoring
  * modest but critical use of black-box monitoring
- the four golden signals (for user-facing system)
  * latency
    + separate the latency of successful requests and that of failed requests
  * traffic
    + for a web service, this is usually HTTP requests per second
    + for a key-value storage system, this might be transactions and retrievals
      per second
  * errors
    + rate of requests that failed
      + examples
        + HTTP `500`
        +  latency over a certain latency threshold
  * saturation
    + how "full" the service is
    + most services need to use indirect signals like CPU utilisation or network
      bandwidth that have a known upper bound
    + measuring 99th percentile response time over a small window (for
      instance, one minute) can give a very early signal of saturation
    + preduction of impending saturation
      + examples
        + database will fill its hard drive in 4 hours
- paging a human if one of the golden signals arises
  * the baseline of monitoring is good
- fundamental philosophy on paging
  * an SRE can only react with a sense of urgency a few times a day before the
    engineer become fatigued
  * every page should be actionable
  * every page response should require intelligence
    + if a page merely merits a robotic response, it should not be a page
  * pages should be about a novel problem or an event that has not been seen
    before

### Chapter 7 The Evolution of Automation at Google

- [text](https://sre.google/sre-book/automation-at-google/)
- automation
  * the main motivation is to scale
  * other benefits
    + better consistency
    + easier to be extended
    + centralises mistakes so that effort of fixing one kind of bug is lower
    + an automated can be ran more frequently potentially or run at times that
      are inconvenient for humans
    + to resolve common faults in a system
      + reduce mean time to repair (MTTR)
    + time saving
    + encapsulation of some tasks
      + anyone can execute it
        + decoupling operator from operation
- automation is meta-software
  * software to act on software
- potential problems with automation
  * turnup automation
    + automations responsible for orchestrating the activation (turn-up) and
      deactivation (turn-down) of network services or IT resources
      + problematic if these automations are separated from the core system
        + suffers from "bit rot"
          + automation does not keep up with the underlying system changes
  * creative use of SSH
    + initial win
    + those free-form scripts became a cholestrol of technical debt
  * automation code, like unit test code, dies when the maintaining team is not
    obsessive about keeping the code in sync with the codebase it covers
    + a product manager whose schedule is not affected by low-quality
      automation will always prioritise new features over simplicity and
      automation
- the most functional tools are usually written by those who use them; a
  similar argument applies to why product development teams benefit from
  keeping at least some operational awareness of their systems in production

### Chapter 8 Release Engineering

- [text](https://sre.google/sre-book/release-engineering/)
- release engineering
  * building and delivering software
  * skills required
    + source code management
    + compilers
    + build configuration languages
    + automated build tools
    + package managers 
    + installers
  * binaries and configurations are built in a reproducible, automated way
    + releases are repeatable and are not "unique snowflakes"
  * work with software engineers and SREs to define all the steps required to
    release software
- measures
  * release velocity
    + how much time it takes for a code change to be deployed into production
- philosophy
  * teams must be self-suffient
    + it does not scale otherwise
  * frequent releases result in fewer changes between versions
    + makes testing and troubleshooting easier
    + "push on green" release model
      + deploy every build that passes all tests
  * ensure consistency repeatability by ensure hermetic builds
    + builds are insensitive to the libraries and other software installed on
      the build machine
      + instead builds depend on known versions of build tools, such as
        compilers, and dependencies, such as libraries
    + the build process is self-contained and must not rely on services that
      are external to the build environment
    + tactic cherry-pick
      + a project built last month would not use this month’s version of the
        compiler if a cherry pick is required
      + Docker can freeze the tools easily
  * gated operations
    + approving source code changes
    + specifying the actions to be performed during the release process
    + creating a new release
    + approving the initial integration proposal and subsequent cherry picks
    + deploying a new release
    + making changes to a build configuration
  * with automated release system produces a report of all changes contained in
    a release, SREs can understand what changes are included in a new release
    of a project; this report can expedite troubleshooting when there are
    problems with a release
- branching
  * all code is checked into `main`
  * release does not created from `main` directly
  * a release is created by branching out from a commit in `main`
    + changes made in the deployment branch would never be merged back into `main`.
  * bug fixes are merged into `main` first and cherry-pick into a deployment branch 
    + this avoids inadvertently picking up unrelated changes submitted to
      `main` since the original build occurred; using this branch and cherry
      pick method, we know the exact contents of each release
- testing
- unit tests are ran against every merge to `main`
  * detect build and test failures quickly
  * decrease the chance that subsequent changes made `main` will cause failures
    during the build performed at release time
- configuration
  * configuration changes are a potential source of instability
  * storing configuration in source control
  * enforcing a strict code review requirement

### Chapter 9 Simplicity

- [text](https://sre.google/sre-book/simplicity/)

## Part III - Practices

- [text](https://sre.google/sre-book/part-III-practices/)

### Chapter 10 Practical Alerting

- [text](https://sre.google/sre-book/practical-alerting/)

### Chapter 11 Being On-Call

- [text](https://sre.google/sre-book/being-on-call/)

### Chapter 12 Effective Troubleshooting

- [text](https://sre.google/sre-book/effective-troubleshooting/)

### Chapter 13 Emergency Response

- [text](https://sre.google/sre-book/emergency-response/)

### Chapter 14 Managing Incidents

- [text](https://sre.google/sre-book/managing-incidents/)

### Chapter 15 Postmortem Culture: Learning from Failure


- [text](https://sre.google/sre-book/postmortem-culture/)

### Chapter 16 Tracking Outages

- [text](https://sre.google/sre-book/tracking-outages/)

### Chapter 17 Testing for Reliability

- [text](https://sre.google/sre-book/testing-reliability/)

### Chapter 18 Software Engineering in SRE

- [text](https://sre.google/sre-book/software-engineering-in-sre/)

### Chapter 19 Load Balancing at the Frontend

- [text](https://sre.google/sre-book/load-balancing-frontend/)

### Chapter 20 Load Balancing in the Datacenter

- [text](https://sre.google/sre-book/load-balancing-datacenter/)

### Chapter 21 Handling Overload

- [text](https://sre.google/sre-book/handling-overload/)

### Chapter 22 Addressing Cascading Failures

- [text](https://sre.google/sre-book/addressing-cascading-failures/)

### Chapter 23 Managing Critical State: Distributed Consensus for Reliability

- [text](https://sre.google/sre-book/managing-critical-state/)

### Chapter 24 Distributed Periodic Scheduling with Cron

- [text](https://sre.google/sre-book/distributed-periodic-scheduling/)

### Chapter 25 Data Processing Pipelines

- [text](https://sre.google/sre-book/data-processing-pipelines/)

### Chapter 26 Data Integrity: What You Read Is What You Wrote

- [text](https://sre.google/sre-book/data-integrity/)

### Chapter 27 Reliable Product Launches at Scale

- [text](https://sre.google/sre-book/reliable-product-launches/)

## Part IV - Management

### Chapter 28 Accelerating SREs to On-Call and Beyond

- [text](https://sre.google/sre-book/accelerating-sre-on-call/)

### Chapter 29 Dealing with Interrupts

- [text](https://sre.google/sre-book/dealing-with-interrupts/)

### Chapter 30 Embedding an SRE to Recover from Operational Overload

- [text](https://sre.google/sre-book/operational-overload/)

### Chapter 31 Communication and Collaboration in SRE

- [text](https://sre.google/sre-book/communication-and-collaboration/)

### Chapter 32 The Evolving SRE Engagement Model

- [text](https://sre.google/sre-book/evolving-sre-engagement-model/)

## Part V - Conclusions

### Chapter 33 Lessons Learned from Other Industries

- [text](https://sre.google/sre-book/lessons-learned/)

### Chapter 34 Conclusion

- [text](https://sre.google/sre-book/conclusion/)

